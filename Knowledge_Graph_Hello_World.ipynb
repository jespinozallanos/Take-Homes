{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledge Graph - Hello World.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDaeljGLdTbZuUExilPHuO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mindyng/Projects/blob/master/Knowledge_Graph_Hello_World.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxzbRJbV0vAU",
        "colab_type": "text"
      },
      "source": [
        "First steps into Knowledge Graph embedding models in PyKeen.\n",
        "\n",
        "[Original](https://github.com/pykeen/pykeen/blob/master/notebooks/hello_world/Hello%20World!.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP_DW2oD8Bnr",
        "colab_type": "text"
      },
      "source": [
        "From this notebook, will be able to complete the following:\n",
        "\n",
        "1. Train a model\n",
        "2. Evaluate how well it learned\n",
        "3. Turn it around and start making  predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zd2fn268E80",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMwTR8ub8Z9M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "d198190a-1a77-4d11-bb9f-3cdce44e5ffe"
      },
      "source": [
        "!pip install pykeen"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pykeen in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from pykeen) (0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from pykeen) (0.8.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pykeen) (1.18.5)\n",
            "Requirement already satisfied: click-default-group in /usr/local/lib/python3.6/dist-packages (from pykeen) (1.2.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.6/dist-packages (from pykeen) (0.5.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pykeen) (0.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pykeen) (4.41.1)\n",
            "Requirement already satisfied: optuna<1.2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pykeen) (1.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pykeen) (1.5.1+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pykeen) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from pykeen) (7.1.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pykeen) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->pykeen) (0.22.2.post1)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from dataclasses-json->pykeen) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from dataclasses-json->pykeen) (0.6.0)\n",
            "Requirement already satisfied: stringcase<2.0.0,==1.2.0 in /usr/local/lib/python3.6/dist-packages (from dataclasses-json->pykeen) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from dataclasses-json->pykeen) (3.6.1)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna<1.2.0,>=1.0.0->pykeen) (3.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna<1.2.0,>=1.0.0->pykeen) (0.15.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna<1.2.0,>=1.0.0->pykeen) (1.4.2)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna<1.2.0,>=1.0.0->pykeen) (1.4.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna<1.2.0,>=1.0.0->pykeen) (4.1.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna<1.2.0,>=1.0.0->pykeen) (1.3.17)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pykeen) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pykeen) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pykeen) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pykeen) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pykeen) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->pykeen) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->pykeen) (2.8.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from typing-inspect>=0.4.0->dataclasses-json->pykeen) (0.4.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from typing-inspect>=0.4.0->dataclasses-json->pykeen) (3.7.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna<1.2.0,>=1.0.0->pykeen) (1.12.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna<1.2.0,>=1.0.0->pykeen) (3.13)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna<1.2.0,>=1.0.0->pykeen) (5.4.5)\n",
            "Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna<1.2.0,>=1.0.0->pykeen) (1.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna<1.2.0,>=1.0.0->pykeen) (2.4.7)\n",
            "Requirement already satisfied: stevedore>=1.20.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna<1.2.0,>=1.0.0->pykeen) (2.0.1)\n",
            "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna<1.2.0,>=1.0.0->pykeen) (0.7.2)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna<1.2.0,>=1.0.0->pykeen) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna<1.2.0,>=1.0.0->pykeen) (1.1.3)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna<1.2.0,>=1.0.0->pykeen) (19.3.0)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna<1.2.0,>=1.0.0->pykeen) (0.4.3)\n",
            "Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna<1.2.0,>=1.0.0->pykeen) (47.3.1)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna<1.2.0,>=1.0.0->pykeen) (1.8.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna<1.2.0,>=1.0.0->pykeen) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna<1.2.0,>=1.0.0->pykeen) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2v6lmzu0Yej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import getpass\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pykeen\n",
        "import torch\n",
        "from pykeen.pipeline import pipeline\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSJt-eH38WFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f3f2dcbe-7a08-4d28-fa85-a48638e88772"
      },
      "source": [
        "print(sys.version)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.9 (default, Apr 18 2020, 01:56:04) \n",
            "[GCC 8.4.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTNXy0U38m2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f015dc92-167e-4879-e190-2955a2ff8004"
      },
      "source": [
        "print(time.asctime())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul  4 18:26:26 2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAwHb0Ur8xrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dd9baad-9251-4997-c0ad-40cebb8b7c1d"
      },
      "source": [
        "print(getpass.getuser())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MgH98Ad8848",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31144322-098b-40d3-b0e0-769ae91ce77f"
      },
      "source": [
        "print(pykeen.get_version(with_git_hash=True))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.1-UNHASHED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn7KUk-TKfcc",
        "colab_type": "text"
      },
      "source": [
        "##Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr9i7AyuGDkW",
        "colab_type": "text"
      },
      "source": [
        "Before training model, let's take a quick look at the Nations dataset to get a breakdown of how dataset is split for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdmTxxYPGCy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ee2b726c-cc5d-4b6f-9991-23303022c9b9"
      },
      "source": [
        "from pykeen.datasets import datasets, Nations\n",
        "\n",
        "nations = Nations()\n",
        "nations.summarize()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nations (create_inverse_triples=False)\n",
            "Name          Entities    Relations    Triples\n",
            "----------  ----------  -----------  ---------\n",
            "Training            14           55       1592\n",
            "Testing             14           41        201\n",
            "Validation          14           45        199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm9P6ga2BTvA",
        "colab_type": "text"
      },
      "source": [
        "## Train a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF_nlS9rBYyG",
        "colab_type": "text"
      },
      "source": [
        "More tutorials to train first model [here](https://pykeen.readthedocs.io/en/latest/first_steps.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3gxP4ZgCGsz",
        "colab_type": "text"
      },
      "source": [
        "Modifications to this:\n",
        "1. swtich model\n",
        "2. add a loss\n",
        "3. add a regularizer\n",
        "4. switch training assumption from sLCWA to LCWA\n",
        "5. Each model does have their own hyper-parameters but PyKEEN tries to have reasonable defaults. Most useful hyperparameter to change is num_epochs in training_kwargs, which is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q05e4AC_9J7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce27ded6-85c8-4f22-bae3-bb7dd3c6ede0"
      },
      "source": [
        "result = pipeline(\n",
        "    dataset = 'Nations',\n",
        "    model = 'RotatE',\n",
        "    training_kwargs=dict(num_epochs=100),\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:pykeen.pipeline:No random seed is specified. Setting to 248040660.\n",
            "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
            "Training epochs on cpu:   0%|          | 0/100 [00:00<?, ?epoch/s]INFO:pykeen.training.training_loop:using stopper: <pykeen.stoppers.stopper.NopStopper object at 0x7fe19c7a24e0>\n",
            "\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.79batch/s]\u001b[A\n",
            "Training epochs on cpu:   1%|          | 1/100 [00:00<00:20,  4.91epoch/s, loss=0.00411, prev_loss=nan]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.32batch/s]\u001b[A\n",
            "Training epochs on cpu:   2%|▏         | 2/100 [00:00<00:20,  4.87epoch/s, loss=0.00396, prev_loss=nan]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.87batch/s]\u001b[A\n",
            "Training epochs on cpu:   3%|▎         | 3/100 [00:00<00:19,  4.88epoch/s, loss=0.00383, prev_loss=0.00396]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.16batch/s]\u001b[A\n",
            "Training epochs on cpu:   4%|▍         | 4/100 [00:00<00:19,  4.83epoch/s, loss=0.00379, prev_loss=0.00383]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 51.78batch/s]\u001b[A\n",
            "Training epochs on cpu:   5%|▌         | 5/100 [00:01<00:19,  4.79epoch/s, loss=0.00369, prev_loss=0.00379]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.50batch/s]\u001b[A\n",
            "Training epochs on cpu:   6%|▌         | 6/100 [00:01<00:19,  4.75epoch/s, loss=0.00373, prev_loss=0.00369]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.38batch/s]\u001b[A\n",
            "Training epochs on cpu:   7%|▋         | 7/100 [00:01<00:19,  4.76epoch/s, loss=0.00365, prev_loss=0.00373]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.62batch/s]\u001b[A\n",
            "Training epochs on cpu:   8%|▊         | 8/100 [00:01<00:19,  4.80epoch/s, loss=0.00379, prev_loss=0.00365]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 58.22batch/s]\u001b[A\n",
            "Training epochs on cpu:   9%|▉         | 9/100 [00:01<00:19,  4.75epoch/s, loss=0.0037, prev_loss=0.00379]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.65batch/s]\u001b[A\n",
            "Training epochs on cpu:  10%|█         | 10/100 [00:02<00:19,  4.69epoch/s, loss=0.00362, prev_loss=0.0037]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 57.17batch/s]\u001b[A\n",
            "Training epochs on cpu:  11%|█         | 11/100 [00:02<00:18,  4.75epoch/s, loss=0.00362, prev_loss=0.00362]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.60batch/s]\u001b[A\n",
            "Training epochs on cpu:  12%|█▏        | 12/100 [00:02<00:18,  4.79epoch/s, loss=0.00364, prev_loss=0.00362]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.70batch/s]\u001b[A\n",
            "Training epochs on cpu:  13%|█▎        | 13/100 [00:02<00:18,  4.78epoch/s, loss=0.0035, prev_loss=0.00364]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.41batch/s]\u001b[A\n",
            "Training epochs on cpu:  14%|█▍        | 14/100 [00:02<00:18,  4.72epoch/s, loss=0.00361, prev_loss=0.0035]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.53batch/s]\u001b[A\n",
            "Training epochs on cpu:  15%|█▌        | 15/100 [00:03<00:17,  4.76epoch/s, loss=0.00363, prev_loss=0.00361]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.13batch/s]\u001b[A\n",
            "Training epochs on cpu:  16%|█▌        | 16/100 [00:03<00:17,  4.80epoch/s, loss=0.00362, prev_loss=0.00363]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.70batch/s]\u001b[A\n",
            "Training epochs on cpu:  17%|█▋        | 17/100 [00:03<00:17,  4.79epoch/s, loss=0.00375, prev_loss=0.00362]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.06batch/s]\u001b[A\n",
            "Training epochs on cpu:  18%|█▊        | 18/100 [00:03<00:17,  4.78epoch/s, loss=0.00358, prev_loss=0.00375]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  71%|███████▏  | 5/7 [00:00<00:00, 49.93batch/s]\u001b[A\n",
            "Training epochs on cpu:  19%|█▉        | 19/100 [00:03<00:17,  4.66epoch/s, loss=0.00363, prev_loss=0.00358]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.53batch/s]\u001b[A\n",
            "Training epochs on cpu:  20%|██        | 20/100 [00:04<00:16,  4.71epoch/s, loss=0.00357, prev_loss=0.00363]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.15batch/s]\u001b[A\n",
            "Training epochs on cpu:  21%|██        | 21/100 [00:04<00:16,  4.72epoch/s, loss=0.00362, prev_loss=0.00357]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.28batch/s]\u001b[A\n",
            "Training epochs on cpu:  22%|██▏       | 22/100 [00:04<00:16,  4.72epoch/s, loss=0.0036, prev_loss=0.00362]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  71%|███████▏  | 5/7 [00:00<00:00, 49.81batch/s]\u001b[A\n",
            "Training epochs on cpu:  23%|██▎       | 23/100 [00:04<00:16,  4.64epoch/s, loss=0.00357, prev_loss=0.0036]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  57%|█████▋    | 4/7 [00:00<00:00, 37.23batch/s]\u001b[A\n",
            "Training epochs on cpu:  24%|██▍       | 24/100 [00:05<00:16,  4.48epoch/s, loss=0.0036, prev_loss=0.00357]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.96batch/s]\u001b[A\n",
            "Training epochs on cpu:  25%|██▌       | 25/100 [00:05<00:16,  4.55epoch/s, loss=0.00348, prev_loss=0.0036]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.90batch/s]\u001b[A\n",
            "Training epochs on cpu:  26%|██▌       | 26/100 [00:05<00:16,  4.60epoch/s, loss=0.00344, prev_loss=0.00348]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.55batch/s]\u001b[A\n",
            "Training epochs on cpu:  27%|██▋       | 27/100 [00:05<00:15,  4.63epoch/s, loss=0.00355, prev_loss=0.00344]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.11batch/s]\u001b[A\n",
            "Training epochs on cpu:  28%|██▊       | 28/100 [00:05<00:15,  4.58epoch/s, loss=0.00357, prev_loss=0.00355]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.48batch/s]\u001b[A\n",
            "Training epochs on cpu:  29%|██▉       | 29/100 [00:06<00:15,  4.58epoch/s, loss=0.00366, prev_loss=0.00357]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  71%|███████▏  | 5/7 [00:00<00:00, 46.52batch/s]\u001b[A\n",
            "Training epochs on cpu:  30%|███       | 30/100 [00:06<00:15,  4.53epoch/s, loss=0.00348, prev_loss=0.00366]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 58.93batch/s]\u001b[A\n",
            "Training epochs on cpu:  31%|███       | 31/100 [00:06<00:14,  4.62epoch/s, loss=0.00347, prev_loss=0.00348]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.93batch/s]\u001b[A\n",
            "Training epochs on cpu:  32%|███▏      | 32/100 [00:06<00:14,  4.59epoch/s, loss=0.00332, prev_loss=0.00347]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 50.35batch/s]\u001b[A\n",
            "Training epochs on cpu:  33%|███▎      | 33/100 [00:07<00:14,  4.56epoch/s, loss=0.00341, prev_loss=0.00332]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.62batch/s]\u001b[A\n",
            "Training epochs on cpu:  34%|███▍      | 34/100 [00:07<00:14,  4.60epoch/s, loss=0.00342, prev_loss=0.00341]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 57.25batch/s]\u001b[A\n",
            "Training epochs on cpu:  35%|███▌      | 35/100 [00:07<00:13,  4.65epoch/s, loss=0.00358, prev_loss=0.00342]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.48batch/s]\u001b[A\n",
            "Training epochs on cpu:  36%|███▌      | 36/100 [00:07<00:13,  4.66epoch/s, loss=0.00347, prev_loss=0.00358]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.70batch/s]\u001b[A\n",
            "Training epochs on cpu:  37%|███▋      | 37/100 [00:07<00:13,  4.69epoch/s, loss=0.00341, prev_loss=0.00347]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.87batch/s]\u001b[A\n",
            "Training epochs on cpu:  38%|███▊      | 38/100 [00:08<00:13,  4.66epoch/s, loss=0.00362, prev_loss=0.00341]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.33batch/s]\u001b[A\n",
            "Training epochs on cpu:  39%|███▉      | 39/100 [00:08<00:12,  4.69epoch/s, loss=0.00347, prev_loss=0.00362]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.66batch/s]\u001b[A\n",
            "Training epochs on cpu:  40%|████      | 40/100 [00:08<00:12,  4.67epoch/s, loss=0.00351, prev_loss=0.00347]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.61batch/s]\u001b[A\n",
            "Training epochs on cpu:  41%|████      | 41/100 [00:08<00:12,  4.68epoch/s, loss=0.00351, prev_loss=0.00351]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  71%|███████▏  | 5/7 [00:00<00:00, 47.76batch/s]\u001b[A\n",
            "Training epochs on cpu:  42%|████▏     | 42/100 [00:08<00:12,  4.61epoch/s, loss=0.00352, prev_loss=0.00351]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.74batch/s]\u001b[A\n",
            "Training epochs on cpu:  43%|████▎     | 43/100 [00:09<00:12,  4.60epoch/s, loss=0.00347, prev_loss=0.00352]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.96batch/s]\u001b[A\n",
            "Training epochs on cpu:  44%|████▍     | 44/100 [00:09<00:12,  4.66epoch/s, loss=0.00344, prev_loss=0.00347]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.49batch/s]\u001b[A\n",
            "Training epochs on cpu:  45%|████▌     | 45/100 [00:09<00:11,  4.69epoch/s, loss=0.0034, prev_loss=0.00344]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.71batch/s]\u001b[A\n",
            "Training epochs on cpu:  46%|████▌     | 46/100 [00:09<00:11,  4.72epoch/s, loss=0.00359, prev_loss=0.0034]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 51.48batch/s]\u001b[A\n",
            "Training epochs on cpu:  47%|████▋     | 47/100 [00:10<00:11,  4.58epoch/s, loss=0.00348, prev_loss=0.00359]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 57.83batch/s]\u001b[A\n",
            "Training epochs on cpu:  48%|████▊     | 48/100 [00:10<00:11,  4.64epoch/s, loss=0.00361, prev_loss=0.00348]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.74batch/s]\u001b[A\n",
            "Training epochs on cpu:  49%|████▉     | 49/100 [00:10<00:10,  4.71epoch/s, loss=0.00351, prev_loss=0.00361]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.30batch/s]\u001b[A\n",
            "Training epochs on cpu:  50%|█████     | 50/100 [00:10<00:10,  4.67epoch/s, loss=0.00358, prev_loss=0.00351]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 57.08batch/s]\u001b[A\n",
            "Training epochs on cpu:  51%|█████     | 51/100 [00:10<00:10,  4.70epoch/s, loss=0.00353, prev_loss=0.00358]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 50.69batch/s]\u001b[A\n",
            "Training epochs on cpu:  52%|█████▏    | 52/100 [00:11<00:10,  4.66epoch/s, loss=0.00337, prev_loss=0.00353]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.07batch/s]\u001b[A\n",
            "Training epochs on cpu:  53%|█████▎    | 53/100 [00:11<00:09,  4.71epoch/s, loss=0.00338, prev_loss=0.00337]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 57.20batch/s]\u001b[A\n",
            "Training epochs on cpu:  54%|█████▍    | 54/100 [00:11<00:09,  4.75epoch/s, loss=0.00362, prev_loss=0.00338]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.56batch/s]\u001b[A\n",
            "Training epochs on cpu:  55%|█████▌    | 55/100 [00:11<00:09,  4.75epoch/s, loss=0.00341, prev_loss=0.00362]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.02batch/s]\u001b[A\n",
            "Training epochs on cpu:  56%|█████▌    | 56/100 [00:11<00:09,  4.76epoch/s, loss=0.00345, prev_loss=0.00341]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.31batch/s]\u001b[A\n",
            "Training epochs on cpu:  57%|█████▋    | 57/100 [00:12<00:09,  4.75epoch/s, loss=0.00341, prev_loss=0.00345]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.56batch/s]\u001b[A\n",
            "Training epochs on cpu:  58%|█████▊    | 58/100 [00:12<00:08,  4.79epoch/s, loss=0.00334, prev_loss=0.00341]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.01batch/s]\u001b[A\n",
            "Training epochs on cpu:  59%|█████▉    | 59/100 [00:12<00:08,  4.77epoch/s, loss=0.0035, prev_loss=0.00334]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.64batch/s]\u001b[A\n",
            "Training epochs on cpu:  60%|██████    | 60/100 [00:12<00:08,  4.71epoch/s, loss=0.00334, prev_loss=0.0035]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.11batch/s]\u001b[A\n",
            "Training epochs on cpu:  61%|██████    | 61/100 [00:13<00:08,  4.69epoch/s, loss=0.00353, prev_loss=0.00334]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.47batch/s]\u001b[A\n",
            "Training epochs on cpu:  62%|██████▏   | 62/100 [00:13<00:08,  4.66epoch/s, loss=0.00345, prev_loss=0.00353]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.89batch/s]\u001b[A\n",
            "Training epochs on cpu:  63%|██████▎   | 63/100 [00:13<00:07,  4.72epoch/s, loss=0.00341, prev_loss=0.00345]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.09batch/s]\u001b[A\n",
            "Training epochs on cpu:  64%|██████▍   | 64/100 [00:13<00:07,  4.72epoch/s, loss=0.00352, prev_loss=0.00341]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.94batch/s]\u001b[A\n",
            "Training epochs on cpu:  65%|██████▌   | 65/100 [00:13<00:07,  4.70epoch/s, loss=0.00344, prev_loss=0.00352]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 51.94batch/s]\u001b[A\n",
            "Training epochs on cpu:  66%|██████▌   | 66/100 [00:14<00:07,  4.58epoch/s, loss=0.00328, prev_loss=0.00344]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.59batch/s]\u001b[A\n",
            "Training epochs on cpu:  67%|██████▋   | 67/100 [00:14<00:07,  4.57epoch/s, loss=0.00324, prev_loss=0.00328]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.64batch/s]\u001b[A\n",
            "Training epochs on cpu:  68%|██████▊   | 68/100 [00:14<00:06,  4.58epoch/s, loss=0.00339, prev_loss=0.00324]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.68batch/s]\u001b[A\n",
            "Training epochs on cpu:  69%|██████▉   | 69/100 [00:14<00:06,  4.58epoch/s, loss=0.00332, prev_loss=0.00339]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.14batch/s]\u001b[A\n",
            "Training epochs on cpu:  70%|███████   | 70/100 [00:14<00:06,  4.49epoch/s, loss=0.00332, prev_loss=0.00332]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.58batch/s]\u001b[A\n",
            "Training epochs on cpu:  71%|███████   | 71/100 [00:15<00:06,  4.55epoch/s, loss=0.00338, prev_loss=0.00332]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.98batch/s]\u001b[A\n",
            "Training epochs on cpu:  72%|███████▏  | 72/100 [00:15<00:06,  4.52epoch/s, loss=0.00337, prev_loss=0.00338]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.21batch/s]\u001b[A\n",
            "Training epochs on cpu:  73%|███████▎  | 73/100 [00:15<00:05,  4.62epoch/s, loss=0.00331, prev_loss=0.00337]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 52.51batch/s]\u001b[A\n",
            "Training epochs on cpu:  74%|███████▍  | 74/100 [00:15<00:05,  4.65epoch/s, loss=0.00337, prev_loss=0.00331]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.99batch/s]\u001b[A\n",
            "Training epochs on cpu:  75%|███████▌  | 75/100 [00:16<00:05,  4.70epoch/s, loss=0.00332, prev_loss=0.00337]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  71%|███████▏  | 5/7 [00:00<00:00, 49.22batch/s]\u001b[A\n",
            "Training epochs on cpu:  76%|███████▌  | 76/100 [00:16<00:05,  4.70epoch/s, loss=0.00328, prev_loss=0.00332]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.63batch/s]\u001b[A\n",
            "Training epochs on cpu:  77%|███████▋  | 77/100 [00:16<00:04,  4.73epoch/s, loss=0.00329, prev_loss=0.00328]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 57.34batch/s]\u001b[A\n",
            "Training epochs on cpu:  78%|███████▊  | 78/100 [00:16<00:04,  4.78epoch/s, loss=0.0033, prev_loss=0.00329]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.90batch/s]\u001b[A\n",
            "Training epochs on cpu:  79%|███████▉  | 79/100 [00:16<00:04,  4.74epoch/s, loss=0.00329, prev_loss=0.0033]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.47batch/s]\u001b[A\n",
            "Training epochs on cpu:  80%|████████  | 80/100 [00:17<00:04,  4.74epoch/s, loss=0.00337, prev_loss=0.00329]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 57.80batch/s]\u001b[A\n",
            "Training epochs on cpu:  81%|████████  | 81/100 [00:17<00:03,  4.77epoch/s, loss=0.00329, prev_loss=0.00337]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  71%|███████▏  | 5/7 [00:00<00:00, 45.78batch/s]\u001b[A\n",
            "Training epochs on cpu:  82%|████████▏ | 82/100 [00:17<00:03,  4.65epoch/s, loss=0.00345, prev_loss=0.00329]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 57.47batch/s]\u001b[A\n",
            "Training epochs on cpu:  83%|████████▎ | 83/100 [00:17<00:03,  4.72epoch/s, loss=0.00329, prev_loss=0.00345]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.30batch/s]\u001b[A\n",
            "Training epochs on cpu:  84%|████████▍ | 84/100 [00:17<00:03,  4.70epoch/s, loss=0.00342, prev_loss=0.00329]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.54batch/s]\u001b[A\n",
            "Training epochs on cpu:  85%|████████▌ | 85/100 [00:18<00:03,  4.67epoch/s, loss=0.0035, prev_loss=0.00342]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.84batch/s]\u001b[A\n",
            "Training epochs on cpu:  86%|████████▌ | 86/100 [00:18<00:02,  4.72epoch/s, loss=0.00325, prev_loss=0.0035]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.88batch/s]\u001b[A\n",
            "Training epochs on cpu:  87%|████████▋ | 87/100 [00:18<00:02,  4.74epoch/s, loss=0.00339, prev_loss=0.00325]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.72batch/s]\u001b[A\n",
            "Training epochs on cpu:  88%|████████▊ | 88/100 [00:18<00:02,  4.69epoch/s, loss=0.00339, prev_loss=0.00339]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 53.05batch/s]\u001b[A\n",
            "Training epochs on cpu:  89%|████████▉ | 89/100 [00:19<00:02,  4.70epoch/s, loss=0.00343, prev_loss=0.00339]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.84batch/s]\u001b[A\n",
            "Training epochs on cpu:  90%|█████████ | 90/100 [00:19<00:02,  4.72epoch/s, loss=0.00336, prev_loss=0.00343]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.00batch/s]\u001b[A\n",
            "Training epochs on cpu:  91%|█████████ | 91/100 [00:19<00:01,  4.77epoch/s, loss=0.00337, prev_loss=0.00336]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  71%|███████▏  | 5/7 [00:00<00:00, 49.50batch/s]\u001b[A\n",
            "Training epochs on cpu:  92%|█████████▏| 92/100 [00:19<00:01,  4.72epoch/s, loss=0.00339, prev_loss=0.00337]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu: 100%|██████████| 7/7 [00:00<00:00, 65.66batch/s]\u001b[A\n",
            "Training epochs on cpu:  93%|█████████▎| 93/100 [00:19<00:01,  4.78epoch/s, loss=0.00318, prev_loss=0.00339]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.52batch/s]\u001b[A\n",
            "Training epochs on cpu:  94%|█████████▍| 94/100 [00:20<00:01,  4.76epoch/s, loss=0.00329, prev_loss=0.00318]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.63batch/s]\u001b[A\n",
            "Training epochs on cpu:  95%|█████████▌| 95/100 [00:20<00:01,  4.77epoch/s, loss=0.00322, prev_loss=0.00329]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 55.97batch/s]\u001b[A\n",
            "Training epochs on cpu:  96%|█████████▌| 96/100 [00:20<00:00,  4.81epoch/s, loss=0.00333, prev_loss=0.00322]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 54.35batch/s]\u001b[A\n",
            "Training epochs on cpu:  97%|█████████▋| 97/100 [00:20<00:00,  4.79epoch/s, loss=0.00334, prev_loss=0.00333]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu: 100%|██████████| 7/7 [00:00<00:00, 66.45batch/s]\u001b[A\n",
            "Training epochs on cpu:  98%|█████████▊| 98/100 [00:20<00:00,  4.79epoch/s, loss=0.00338, prev_loss=0.00334]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 56.55batch/s]\u001b[A\n",
            "Training epochs on cpu:  99%|█████████▉| 99/100 [00:21<00:00,  4.82epoch/s, loss=0.00342, prev_loss=0.00338]\n",
            "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
            "Training batches on cpu:  86%|████████▌ | 6/7 [00:00<00:00, 51.82batch/s]\u001b[A\n",
            "Training epochs on cpu: 100%|██████████| 100/100 [00:21<00:00,  4.69epoch/s, loss=0.00339, prev_loss=0.00342]\n",
            "INFO:pykeen.evaluation.evaluator:Starting batch_size search for evaluation now...\n",
            "INFO:pykeen.evaluation.evaluator:Concluded batch_size search with batch_size=201.\n",
            "Evaluating on cpu: 100%|██████████| 201/201 [00:00<00:00, 10.9ktriple/s]\n",
            "INFO:pykeen.evaluation.evaluator:Evaluation took 0.02s seconds\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omEK2E8BDJWz",
        "colab_type": "text"
      },
      "source": [
        "At this point, if on local machine: Save trained model (including triples and all learned parameters), results from training and all experimental metadata. However, since in cloud notebook, will do this another time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beCGqNMlF0Y9",
        "colab_type": "text"
      },
      "source": [
        "## Now, let's look at the model in more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvPUVZkqC9Q7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "08a14c20-642c-4eb3-ba4b-da7fd42c3d87"
      },
      "source": [
        "model = result.model\n",
        "model"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RotatE(\n",
              "  (loss): MarginRankingLoss()\n",
              "  (regularizer): NoRegularizer()\n",
              "  (entity_embeddings): Embedding(14, 400)\n",
              "  (relation_embeddings): Embedding(55, 400)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP6COnsuHHt4",
        "colab_type": "text"
      },
      "source": [
        "## Looking at the triples (subject, predicate, object):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdhjeBUaF-_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "af1936b3-7dc6-4b93-c64f-151ed1ef5a1b"
      },
      "source": [
        "tf = model.triples_factory\n",
        "tf"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TriplesFactory(path=\"/usr/local/lib/python3.6/dist-packages/pykeen/datasets/nations/train.txt\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg0QpFepHUMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8e538cd-d61d-4fbf-df03-18fb306409f0"
      },
      "source": [
        "tf.num_entities, tf.num_relations, tf.num_triples"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, 55, 1592)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Gh45wyHdFW",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCdf6o7pHg4e",
        "colab_type": "text"
      },
      "source": [
        "(Check that the loss is descending and not oscillating)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Umc4bQTHbcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "66e8eb6b-fe89-43f9-9315-b8b3d0ce8aab"
      },
      "source": [
        "result.plot_losses()\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEqQq4KrHyMT",
        "colab_type": "text"
      },
      "source": [
        "In general, there is a downward trend, but within short windows of time there are upticks including the one at 100th epoch at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vGOmyQqIMkS",
        "colab_type": "text"
      },
      "source": [
        "Things to keep in mind:\n",
        "\n",
        "\n",
        "*   adjusted mean rank is between [0,1]. Closer to 1 is best!\n",
        "*   mean rank is a positive integer with a bound based on number of entities. Closer to 0 is best!\n",
        "*   hits@k is reported between [0,1] and intepreted as a %. Closer to 1 is best!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeXHZql7IL5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "2484241c-26bb-4aa3-b21b-9f9b9a01b72a"
      },
      "source": [
        "result.metric_results.to_df()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Metric</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>avg</td>\n",
              "      <td>adjusted_mean_rank</td>\n",
              "      <td>0.837563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>best</td>\n",
              "      <td>mean_rank</td>\n",
              "      <td>3.902985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>best</td>\n",
              "      <td>mean_reciprocal_rank</td>\n",
              "      <td>0.463393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>best</td>\n",
              "      <td>hits_at_1</td>\n",
              "      <td>0.263682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>best</td>\n",
              "      <td>hits_at_3</td>\n",
              "      <td>0.544776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>best</td>\n",
              "      <td>hits_at_5</td>\n",
              "      <td>0.753731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>best</td>\n",
              "      <td>hits_at_10</td>\n",
              "      <td>0.960199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>worst</td>\n",
              "      <td>mean_rank</td>\n",
              "      <td>3.902985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>worst</td>\n",
              "      <td>mean_reciprocal_rank</td>\n",
              "      <td>0.463393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>worst</td>\n",
              "      <td>hits_at_1</td>\n",
              "      <td>0.263682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>worst</td>\n",
              "      <td>hits_at_3</td>\n",
              "      <td>0.544776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>worst</td>\n",
              "      <td>hits_at_5</td>\n",
              "      <td>0.753731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>worst</td>\n",
              "      <td>hits_at_10</td>\n",
              "      <td>0.960199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>avg</td>\n",
              "      <td>mean_rank</td>\n",
              "      <td>3.902985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>avg</td>\n",
              "      <td>mean_reciprocal_rank</td>\n",
              "      <td>0.463393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>avg</td>\n",
              "      <td>hits_at_1</td>\n",
              "      <td>0.263682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>avg</td>\n",
              "      <td>hits_at_3</td>\n",
              "      <td>0.544776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>avg</td>\n",
              "      <td>hits_at_5</td>\n",
              "      <td>0.753731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>avg</td>\n",
              "      <td>hits_at_10</td>\n",
              "      <td>0.960199</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Type                Metric     Value\n",
              "0     avg    adjusted_mean_rank  0.837563\n",
              "1    best             mean_rank  3.902985\n",
              "2    best  mean_reciprocal_rank  0.463393\n",
              "3    best             hits_at_1  0.263682\n",
              "4    best             hits_at_3  0.544776\n",
              "5    best             hits_at_5  0.753731\n",
              "6    best            hits_at_10  0.960199\n",
              "7   worst             mean_rank  3.902985\n",
              "8   worst  mean_reciprocal_rank  0.463393\n",
              "9   worst             hits_at_1  0.263682\n",
              "10  worst             hits_at_3  0.544776\n",
              "11  worst             hits_at_5  0.753731\n",
              "12  worst            hits_at_10  0.960199\n",
              "13    avg             mean_rank  3.902985\n",
              "14    avg  mean_reciprocal_rank  0.463393\n",
              "15    avg             hits_at_1  0.263682\n",
              "16    avg             hits_at_3  0.544776\n",
              "17    avg             hits_at_5  0.753731\n",
              "18    avg            hits_at_10  0.960199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h34_divmJOVE",
        "colab_type": "text"
      },
      "source": [
        "Metrics are based on loss from test set. \n",
        "\n",
        "Best to look at avg Type and adjusted_mean_rank metric which is only seen in avg Type. And the percentage is in the low 80's which is not bad! Remember that the dataset is small and we do have strong signal. 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0SQrbf3KSpP",
        "colab_type": "text"
      },
      "source": [
        "##Now, turning it around to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVXdAvQ5Ko3j",
        "colab_type": "text"
      },
      "source": [
        "It is difficult to interpret KGEMs statistically. So it is best to sort order the predictions by their scores. All interaction functions in PyKeen have been implemented such that higher the score (less negative the score is), more likely that a triple is to be true.\n",
        "\n",
        "And one last thing before predicting will be to create word clouds of entities and relations in the Nations dataset. Size corresponds to frequency of word appearing in triples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP3jv3HaHq3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a4dc63fc-6d7c-4cb4-937d-39a506edf1d8"
      },
      "source": [
        "tf.entity_word_cloud()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:pykeen.triples.triples_factory:Could not import module `word_cloud`. Try installing with `pip install git+ssh://git@github.com/kavgan/word_cloud.git`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKwJkUwkL6Kt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5cb68483-79f9-4e07-cd1a-7f6c369b60e7"
      },
      "source": [
        "tf.relation_word_cloud()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:pykeen.triples.triples_factory:Could not import module `word_cloud`. Try installing with `pip install git+ssh://git@github.com/kavgan/word_cloud.git`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO8ccdx8Mb4K",
        "colab_type": "text"
      },
      "source": [
        "Who do we predict Brazil to have a conference with?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAkNRT_BMGSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "61973cbb-9831-4d16-8a3a-d49df56f3b6f"
      },
      "source": [
        "model.predict_tails('brazil', 'conferences')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tail_id</th>\n",
              "      <th>tail_label</th>\n",
              "      <th>score</th>\n",
              "      <th>novel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>usa</td>\n",
              "      <td>-4.690113</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>uk</td>\n",
              "      <td>-4.917130</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>ussr</td>\n",
              "      <td>-4.928981</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>india</td>\n",
              "      <td>-5.255620</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>netherlands</td>\n",
              "      <td>-5.287464</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>poland</td>\n",
              "      <td>-5.472327</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>egypt</td>\n",
              "      <td>-5.476140</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>cuba</td>\n",
              "      <td>-5.560370</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>china</td>\n",
              "      <td>-5.601460</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-5.673203</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>israel</td>\n",
              "      <td>-5.714159</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>indonesia</td>\n",
              "      <td>-5.829195</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>jordan</td>\n",
              "      <td>-6.215493</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>burma</td>\n",
              "      <td>-6.364753</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    tail_id   tail_label     score  novel\n",
              "12       12          usa -4.690113   True\n",
              "11       11           uk -4.917130  False\n",
              "13       13         ussr -4.928981   True\n",
              "5         5        india -5.255620   True\n",
              "9         9  netherlands -5.287464  False\n",
              "10       10       poland -5.472327  False\n",
              "4         4        egypt -5.476140  False\n",
              "3         3         cuba -5.560370   True\n",
              "2         2        china -5.601460  False\n",
              "0         0       brazil -5.673203  False\n",
              "7         7       israel -5.714159  False\n",
              "6         6    indonesia -5.829195  False\n",
              "8         8       jordan -6.215493  False\n",
              "1         1        burma -6.364753  False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia1W5cRAMvTC",
        "colab_type": "text"
      },
      "source": [
        "Now, who do we predict to have a conference with Brazil?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Xg5YdkMkCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "523ae043-2cad-4e1c-dbca-7990fef59b2f"
      },
      "source": [
        "model.predict_heads('conferences', 'brazil')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>head_id</th>\n",
              "      <th>head_label</th>\n",
              "      <th>score</th>\n",
              "      <th>novel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>usa</td>\n",
              "      <td>-4.826950</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>uk</td>\n",
              "      <td>-5.067481</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>india</td>\n",
              "      <td>-5.091891</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>ussr</td>\n",
              "      <td>-5.095834</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>netherlands</td>\n",
              "      <td>-5.208021</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>poland</td>\n",
              "      <td>-5.223291</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>egypt</td>\n",
              "      <td>-5.228087</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>cuba</td>\n",
              "      <td>-5.483738</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>china</td>\n",
              "      <td>-5.668639</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>brazil</td>\n",
              "      <td>-5.673203</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>indonesia</td>\n",
              "      <td>-5.860926</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>israel</td>\n",
              "      <td>-5.921488</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>jordan</td>\n",
              "      <td>-6.365452</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>burma</td>\n",
              "      <td>-6.531160</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    head_id   head_label     score  novel\n",
              "12       12          usa -4.826950   True\n",
              "11       11           uk -5.067481  False\n",
              "5         5        india -5.091891   True\n",
              "13       13         ussr -5.095834   True\n",
              "9         9  netherlands -5.208021  False\n",
              "10       10       poland -5.223291   True\n",
              "4         4        egypt -5.228087  False\n",
              "3         3         cuba -5.483738   True\n",
              "2         2        china -5.668639  False\n",
              "0         0       brazil -5.673203  False\n",
              "6         6    indonesia -5.860926  False\n",
              "7         7       israel -5.921488  False\n",
              "8         8       jordan -6.365452  False\n",
              "1         1        burma -6.531160  False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYHgI-5OM3B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}